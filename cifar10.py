from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
import numpy as np
import keras.applications.resnet50
from keras.datasets import cifar10,mnist
import os
import keras.backend as K

from keras.layers import BatchNormalization,Conv2D,Activation,MaxPooling2D,Dense
from keras.layers import AveragePooling2D, Input, Flatten
from keras.models import Model
from math import ceil


#A single Unit
def UnitCell(x, channels, kernel_size=[3, 3], strides=(1, 1)):
    y = BatchNormalization(scale=True, momentum=0.95)(x)
    y = Activation("relu")(y)
    y = Conv2D(channels, kernel_initializer='he_normal', kernel_size=kernel_size, strides=(strides), padding="same")(y)

    return y

#The network
def FastNet(input_shape, num_classes=10):
    inputs = Input(input_shape)

    y = UnitCell(inputs, 64)
    y = UnitCell(y, 128)
    y = UnitCell(y, 128)
    y = UnitCell(y, 128)
    y = MaxPooling2D(pool_size=(2, 2), strides=[2, 2])(y)

    y = UnitCell(y, 128)
    y = UnitCell(y, 128)

    y = UnitCell(y, 128)
    y = MaxPooling2D(pool_size=(2, 2), strides=[2, 2])(y)

    y = UnitCell(y, 128)
    y = UnitCell(y, 128)

    y = UnitCell(y, 128)
    y = MaxPooling2D(pool_size=(2, 2), strides=[2, 2])(y)

    y = UnitCell(y, 128)
    y = UnitCell(y, 128)
    y = MaxPooling2D(pool_size=(2, 2), strides=[2, 2])(y)

    y = UnitCell(y, 128, kernel_size=[1, 1])
    y = UnitCell(y, 128, kernel_size=[1, 1])
    y = UnitCell(y, 128, kernel_size=[1, 1])
    y = AveragePooling2D(pool_size=(2, 2))(y)
    y = Flatten()(y)
    outputs = Dense(num_classes,
                    activation='softmax')(y)

    model = Model(inputs=inputs, outputs=outputs, name="FastNet")
    return model



#Load the CIFAR10 data.
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Input image dimensions
input_shape = x_train.shape[1:]

num_classes = 10
batch_size = 128

# Normalize data.
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

x_train = x_train.astype('float32')
x_train = (x_train - x_train.mean(axis=0)) / (x_train.std(axis=0))
x_test = x_test.astype('float32')
x_test = (x_test - x_test.mean(axis=0)) / (x_test.std(axis=0))

num_train_samples = x_train.shape[0]
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
print('y_train shape:', y_train.shape)

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = FastNet(input_shape, num_classes=10)

save_direc = os.path.join(os.getcwd(), 'cifar10_saved_modelsbest')

model_name = 'cifar10_model.{epoch:03d}.h5'
if not os.path.isdir(save_direc):
    os.makedirs(save_direc)
filepath = os.path.join(save_direc, model_name)

# Prepare callbacks for model saving and for learning rate adjustment.
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=True)


def lr_schedule(epoch):
    """Learning Rate Schedule
    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.
    # Arguments
        epoch (int): The number of epochs
    # Returns
        lr (float32): learning rate
    """

    lr = 1e-3
    if epoch > 180:
        lr *= 0.5e-3
    elif epoch > 160:
        lr *= 1e-3
    elif epoch > 120:
        lr *= 1e-2
    elif epoch > 80:
        lr *= 1e-1

    print('Learning rate: ', lr)

    return lr


model.compile(loss='categorical_crossentropy',
              optimizer=Adam(lr=lr_schedule(0)),
              metrics=['accuracy'])
model.summary()

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

callbacks = [checkpoint, lr_reducer, lr_scheduler]

# preprocessing and realtime data augmentation:
datagen = ImageDataGenerator(rotation_range=10,
                             width_shift_range=5. / 32,
                             height_shift_range=5. / 32,
                             horizontal_flip=True)

# Compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied).
datagen.fit(x_train)

epochs = 200
steps_per_epoch = ceil(num_train_samples/batch_size)
# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                    validation_data=(x_test, y_test),
                    epochs=epochs,steps_per_epoch=steps_per_epoch, verbose=1, workers=4,
                    callbacks=callbacks)

# Score trained model.
scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])
